# LBOB AI Nginx Configuration - Production Load Balancing & SSL
# Enterprise-grade reverse proxy with failover and monitoring

upstream lbob_backend {
    # Load balancing with health checks
    server 127.0.0.1:8000 max_fails=3 fail_timeout=30s;
    # Add additional workers if needed:
    # server 127.0.0.1:8001 max_fails=3 fail_timeout=30s backup;

    keepalive 32;
}

upstream ollama_backend {
    server 127.0.0.1:11434 max_fails=2 fail_timeout=60s;
    keepalive 8;
}

# Rate limiting
limit_req_zone $binary_remote_addr zone=api_limit:10m rate=10r/s;
limit_req_zone $binary_remote_addr zone=ai_limit:10m rate=2r/s;

# Main LBOB AI Server
server {
    listen 80;
    listen [::]:80;
    server_name 192.168.1.70 lbob.local;

    # Security headers
    add_header X-Frame-Options DENY always;
    add_header X-Content-Type-Options nosniff always;
    add_header X-XSS-Protection "1; mode=block" always;
    add_header Referrer-Policy "strict-origin-when-cross-origin" always;

    # Logging
    access_log /var/log/nginx/lbob_access.log combined;
    error_log /var/log/nginx/lbob_error.log warn;

    # Static files (with caching)
    location /static/ {
        alias /opt/aibrainframe_claude/assets/;
        expires 1h;
        add_header Cache-Control "public, no-transform";

        # Security for static files
        location ~* \.(js|css|png|jpg|jpeg|gif|ico|svg)$ {
            expires 1y;
            add_header Cache-Control "public, immutable";
        }
    }

    # Health check endpoint (no rate limiting)
    location /health {
        proxy_pass http://lbob_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Quick timeout for health checks
        proxy_connect_timeout 5s;
        proxy_send_timeout 5s;
        proxy_read_timeout 5s;
    }

    # API endpoints with rate limiting
    location /api/ {
        limit_req zone=api_limit burst=20 nodelay;

        proxy_pass http://lbob_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Standard timeouts
        proxy_connect_timeout 10s;
        proxy_send_timeout 30s;
        proxy_read_timeout 60s;
    }

    # AI conversation endpoints (longer timeouts, stricter rate limiting)
    location ~ ^/(conversations|users)/ {
        limit_req zone=ai_limit burst=5 nodelay;

        proxy_pass http://lbob_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        # Extended timeouts for AI processing
        proxy_connect_timeout 15s;
        proxy_send_timeout 60s;
        proxy_read_timeout 180s;  # AI responses can take time
    }

    # All other API requests
    location / {
        limit_req zone=api_limit burst=10 nodelay;

        proxy_pass http://lbob_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;

        proxy_connect_timeout 10s;
        proxy_send_timeout 30s;
        proxy_read_timeout 60s;
    }
}

# Ollama AI Backend (internal only)
server {
    listen 11435;  # Different port for proxied access
    server_name 127.0.0.1;

    # Only allow internal access
    allow 127.0.0.1;
    allow 192.168.1.0/24;
    deny all;

    access_log /var/log/nginx/ollama_access.log combined;
    error_log /var/log/nginx/ollama_error.log warn;

    location / {
        limit_req zone=ai_limit burst=3 nodelay;

        proxy_pass http://ollama_backend;
        proxy_set_header Host $host;
        proxy_set_header X-Real-IP $remote_addr;

        # Extended timeouts for AI model operations
        proxy_connect_timeout 30s;
        proxy_send_timeout 120s;
        proxy_read_timeout 300s;  # AI model inference can take several minutes

        # Increase buffer sizes for large AI responses
        proxy_buffering on;
        proxy_buffer_size 128k;
        proxy_buffers 4 256k;
        proxy_busy_buffers_size 256k;
    }
}

# Monitoring and Status Page
server {
    listen 8080;
    server_name 127.0.0.1;

    # Only allow internal access
    allow 127.0.0.1;
    allow 192.168.1.0/24;
    deny all;

    access_log /var/log/nginx/monitor_access.log combined;

    # Nginx status
    location /nginx_status {
        stub_status on;
        access_log off;
    }

    # Health monitoring dashboard
    location /health-dashboard {
        alias /opt/aibrainframe_claude/deployment/monitoring/dashboard/;
        index health.html;
    }

    # Live logs
    location /logs {
        alias /var/log/lbob/;
        autoindex on;
        autoindex_exact_size off;
        autoindex_localtime on;
    }
}